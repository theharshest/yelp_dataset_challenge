# -*- coding: utf-8 -*-
"""
Utilities for doing "walk forward cross validation".

Created on Sun Nov 30 09:51:14 2014

@author: John Maloney
"""

import time
import feat_info as fi
import datautils as du
import jsonutils as ju
import numpy as np
import sklearn.metrics as metrics
import sklearn.grid_search as grid_search
import sklearn.decomposition as decomp
import sklearn.preprocessing as prep

'''
Estimate the generalization error rate using the "walk forward cross validation"
approach.

Inputs:

  clf:
    the classifier to be tested

  param_grid:
    dictionary with parameters names (string) as keys and lists of parameter
    settings to try as values, or a list of such dictionaries, in which case
    the grids spanned by each dictionary in the list are explored. This enables
    searching over any sequence of parameter settings

  all_buses:
    all the business objects available for generating train and test datasets

  all_reviews:
    all the review objects available for generating train and test datasets

  all_tips:
    all the tip objects available for generating train and test datasets

  all_senti:
    matrix containing all sentiment ranks to consider for the dataset

  init_pdate:
    the initial prediction date to use (in seconds since the epoch)

  time_delta:
    the amount of time between training prediction date and test prediction
    date (in seconds)

  feat_info: (optional)
    the list of features to use for classification (default is feat_info.data_feat_info)

  std_data: (optional)
    if True then the data is standardized to have mean zero and standard deviation
    one (default is True)

  usamp: (optional)
    if True then the "still open" class is under-sampled (default is True)

  states: (optional)
    list of the states to include in the data set, if the parameter is None
    then all states are included (default is None)

  binary: (optional)
    specifies the list of classes to treat as the positive class in a binary
    classification problem, the remaining classes are treated as the negative
    class, if not specified then data is generated for a multi-class
    classification problem (default is None)

  reg: (optional)
    indicaes that the "classifier" is actually a regressor (default is False)

  pca: (optional)
    indicates whether PCA should be used to reduce the dimension of the data:
      pca < 0 - don't use PCA (default)
      pca = 0 - use PCA without limiting the number of components
      pca > 0 - use PCA and limit the number of components to the value specified

Outputs:

  results:
    a list of tuples, one tuple for each round of cross validation, the first
    element in each tuple provides the true values for that round's test examples
    and the second element provides the predictions generated by the classifier
    on that round's test examples
'''
def wfcv(clf, param_grid, all_buses, all_reviews, all_tips, all_senti, init_pdate, time_delta,
         feat_info=fi.data_feat_info, std_data=True, usamp=True, binary=None, reg=False, pca=-1,
         states=None):
    # find the earliest and latest review dates
    start_date = int(time.time())
    end_date = 0
    for bus in all_buses:
        first_review_date = bus[fi.first_review_date]
        last_review_date = bus[fi.last_review_date]
        if (first_review_date < start_date):
            start_date = first_review_date
        if (last_review_date > end_date):
            end_date = last_review_date

    # print out earliest and latest dates
    print('Earliest review date: %s' % du.date2str(du.int2date(start_date)))
    print('Latest review date:   %s' % du.date2str(du.int2date(end_date)))
    
    # initialize the "prediction date"
    pdate = init_pdate

    # create variables for the training data - it will be populated later
    X_train_orig,y_train = None,None

    # generate the first data set
    buses_test = du.gen_dataset(pdate, all_buses, all_reviews, all_tips, all_senti, usamp=usamp, states=states, binary=binary, reg=reg)
    if (reg):
        # extract the target value as the y values for regression
        X_test_orig,y_test = ju.json2xy(buses_test, feat_info, fi.target, std=False)
    else:
        # extract the label value as the y values for classification
        X_test_orig,y_test = ju.json2xy(buses_test, feat_info, fi.label, std=False)
    
    print('Number of attributes in data set: %d' % X_test_orig.shape[1])

    # initialize the stop_date threshold
    stop_date = end_date - 2*time_delta

    # create list to hold results
    results = []

    # configure scoring metric to be used during grid search and feature selection
    if (usamp):
        # if class sizes are balanced then use accuracy
        scorer = 'accuracy'
    else:
        # if class sizes are unbalanced then use f1 score
        scorer = 'f1'

    # perform "walk forward cross validation"
    while (pdate <= stop_date):
        print('\n===================================================================')
        print("Train estimator using train set with prediction date %s:" % du.date2str(du.int2date(pdate)))
        # update the prediction date for the this round
        pdate = pdate + time_delta

        # use current test set as training set for this round
        X_train_orig = X_test_orig
        y_train = y_test

        # generate a new test set for this round
        buses_test = du.gen_dataset(pdate, all_buses, all_reviews, all_tips, all_senti, usamp=usamp, states=states, binary=binary, reg=reg)
        if (reg):
            # extract the target value as the y values for regression
            X_test_orig,y_test = ju.json2xy(buses_test, feat_info, fi.target, std=False)
        else:
            # extract the label value as the y values for classification
            X_test_orig,y_test = ju.json2xy(buses_test, feat_info, fi.label, std=False)

        # by default, use the original untransformed X data
        # - X_train & X_test will contain the transformed data (if any transformation is done)
        X_train = X_train_orig
        X_test = X_test_orig

        # ===========================================
        # apply any requested data transformations

        # standardize the data
        # See http://scikit-learn.org/stable/modules/preprocessing.html
        if (std_data):
            print('  Standardize the data...')
            # scaler is trained on training set
            scaler = prep.StandardScaler().fit(X_train_orig)
            # scaler is used to transform both train and test data
            X_train = scaler.transform(X_train_orig)
            X_test = scaler.transform(X_test_orig)

        # reduce the dimension of the data using PCA
        # See http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html#example-applications-face-recognition-py
        if (pca >= 0):
            print('  Reduce dimension using PCA...')
            if (pca == 0):
                pca = None
            rand_pca = decomp.RandomizedPCA(n_components=pca, whiten=True)
            # fit PCA on the training data
            rand_pca.fit(X_train)
            # transform train and test sets using PCA
            X_train = rand_pca.transform(X_train)
            X_test  = rand_pca.transform(X_test)
            print('    featues remaining after PCA: %d' % X_train.shape[1])

        # data transformations complete
        # ===========================================

        # use grid search to train and test the classifier:
        # - see http://scikit-learn.org/stable/auto_examples/grid_search_digits.html#example-grid-search-digits-py

        if (param_grid):
            # train the classifier using grid search
            gs = grid_search.GridSearchCV(clf, param_grid, n_jobs=-1, scoring=scorer)
            #gs = grid_search.GridSearchCV(clf, param_grid, scoring=scorer)
        else:
            # use the classifier/regressor without grid search
            gs = clf

        print '\nTraining the estimator...'
        gs.fit(X_train, y_train)

        if (param_grid):
            print("\nBest parameters set found on train set:\n")
            print(gs.best_estimator_)
            print("\nGrid scores on train set:\n")
            for params, mean_score, scores in gs.grid_scores_:
                print("  %0.3f (+/-%0.03f) for %r"
                      % (mean_score, scores.std() / 2, params))

        # if using RFE - print out number of features selected
        # TBD

        # collect predictions from the classifier
        print '\nTesting the estimator...'
        y_pred = gs.predict(X_test)

        print("\nResults for test set with prediction date %s:\n" % du.date2str(du.int2date(pdate)))
        if (reg):
            # print out explained variance score, mean absolute error, mean squared
            # error and R-squared metrics
            print_reg_metrics(y_test, y_pred)
        else:
            # print out the confusion matrix
            cm = metrics.confusion_matrix(y_test, y_pred)
            print_cm(cm)

        #print("\nScores on evaluation set:\n")
        #print(metrics.classification_report(y_test, y_pred, target_names=fi.class_names))

        # save results
        results.append((y_test, y_pred))
    #end while

    # return the true values and predictions for each round
    return results
#end wfocv

'''
Print out the confusion matrix.
'''
def print_cm(cm):
    dashes = "-------"
    # generate & print header
    K = cm.shape[0]
    hdr1 = "|" + "".center(9) + "|"
    hdr2 = "|" + dashes.center(9) + "|"
    for k in xrange(K):
        hdr1 += ("class %d" % k).center(9) + "|"
        hdr2 += (dashes).center(9) + "|"
    hdr1 += ("totals").center(9) + "|"
    hdr2 += (dashes).center(9) + "|"
    print(hdr2)
    print(hdr1)
    print(hdr2)
    # generate & print each row
    for j in xrange(K):
        row = "|" + ("class %d" % j).center(9) +"|"
        row_tot = np.sum(cm[j,:], dtype=float)
        for k in xrange(K):
            pcnt = (float(cm[j,k])/row_tot)*100
            row += ("%6.2f%%" % pcnt).center(9) + "|"
        row += ("%6d" % np.sum(cm[j,:])).center(9) + "|"
        print(row)
    # generate and print bottom sum row
    print(hdr2)
    totals = np.sum(cm,axis=0)
    row = "|" + "totals".center(9) + "|"
    for t in totals:
        row += ("%6d" % t).center(9) + "|"
    row += ("%6d" % np.sum(np.sum(cm))).center(9) + "|"
    print(row)
    print(hdr2)

'''
Print out explained variance score, mean absolute error, mean squared error and
R-squared metrics
'''
def print_reg_metrics(y_test, y_pred):
    print '%s  %s' % ('metric'.center(20), 'value'.center(12))
    print '--------------------  ------------'
    print 'explained variance:  %12.3f' % metrics.explained_variance_score(y_test, y_pred)
    print 'mean absolute error: %12.3f' % metrics.mean_absolute_error(y_test, y_pred)
    print 'mean squared error:  %12.3f' % metrics.mean_squared_error(y_test, y_pred)
    print 'R-squared score:     %12.3f' % metrics.r2_score(y_test, y_pred)